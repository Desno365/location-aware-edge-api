\chapter{Existing Solutions}
\label{ch:existing-solutions}

In this Chapter we present the current frameworks publicly available in the industry to perform serverless computations on the edge on a large scale. We also show the latest frameworks that allow to setup a FaaS infrastructure.



\section{Serverless Edge Computing}

In this section we show the serverless frameoworks made publicly available by the industry to perform edge computations. For each of them we analyze the stateful support that the company offers.


\subsection{AWS Lambda@Edge}
With AWS Lambda@Edge it is possible to run code in a serverless manner on the edge network of AWS \cite{aws-lambda-at-edge}. Currently the only stateful support they offer publicly is with CloudFront, which is the Amazon Content Delivery Network. Therefore with this system, only caching, stateless or forwarding use cases can be fulfilled.


\subsection{Cloudflare Workers}
Cloudflare Workers allow developers to run serverless code across the globe on the edge network of Cloudflare \cite{cloudflare-workers}. Workers are severely capped in terms of CPU and memory usage (max 50ms CPU time limit and max 128MB of RAM), so they are not intended for a CPU intensive task, but they are extremely fast even during cold starts (a cold start is the phenomenon in which a function that was not used in a long time need to be re-instantiated). Workers in fact use an innovative technology to run code in \textit{isolates} instead of \textit{containers}, these \textit{isolates} work in a way similar to the sandbox of a web browser (e.g., Chrome) when opening a new tab and allows to have cold starts that last just a few milliseconds.

For Workers, Cloudflare offers publicly two types of stateful support: the Cloudflare cache system and the Workers KV. The first one is a cache support which exploits the Cloudflare Content Delivery Network, while Workers KV is a global key-value data store. Every write performed on Worker KV is propagated in an eventual consistent way to all the other edge locations, therefore Workers KV is intended for use cases with frequently read but infrequently written values.


\subsection{Akamai EdgeWorkers}
A competitor of Cloudflare is Akamai, and in fact the two companies provide a very similar service: also Akamai offers serverless computation on the edge equipped with the possibility of storing data with the Akamai EdgeKV, a global key-value DB with eventual consistent writes, perfect for use cases with number of reads greatly larger than the number of writes \cite{akamai-edge-workers}.


\subsection{Appfleet}
Appfleet allows to easily deploy long-running containers in multiple locations across the globe, with the goal of running the services closer to the users \cite{appfleet}. Unfortunately their network can't be defined an edge network since is currently composed of only 5 locations.


\section{FaaS Platforms}
We have seen in the previous section that the FaaS paradigm is the most used paradigm to allow computations on the edge in the industry, while long-running solutions are not present, this is expected since the FaaS paradigm can allow to reach an high efficiency which in the edge is essential.

Unfortunately current frameworks do not provide stateful support for use cases with frequent write operations, we saw in fact support only for caching, stateless, forwarding or infrequent-write use cases.
If we apply the frameworks seen in the previous section to the use cases we collected and presented in Chapter \ref{ch:problem_formulation}, we can fulfill only a few use cases like the \textit{"Video Upload"} use case which is stateless, and the \textit{"Industrial IoT Data Compression"} use case which can employ a stateless compression. All the other use cases present an abundant rate of write operations, making the available frameworks unsuitable for the tasks.

Considering this situation in the industry and the need to be efficient we believe researchers should raise the effort in studying systems that can run on demand on the edge, without the need of using long-running containers. 
In view of the fact that current available frameworks do not provide write-frequent stateful support we tried to think a new solution which can fulfill the use cases we collected.
But before thinking about a new solution we studied the platforms that allow to setup a FaaS infrastructure, with the idea in mind to build a prototype of our solution on this infrastructure.

Here we present the most used platforms that we found.


\subsection{Apache OpenWhisk}
Apache OpenWhisk allows to run functions with support to many different languages (e.g., Go, Java, NodeJS, .NET, PHP, Python, Scala, etc...). The project has a very active community and updates are provided periodically.
The architecture used by OpenWhisk is quite complex and internally uses a document-oriented database (\textit{CouchDB}) and a messaging platform (\textit{Kafka}) to process requests that then are forwarded to the \textit{Invoker} which runs the code inside a Docker container \cite{openwhisk-architecture}.

Apache OpenWhisk supports extreme levels of scalability, however this come with the cost of size and burdensome, in fact just running the core components, before running any actions, would require about 2.5 GB of RAM \cite{lean-openwhisk}. This make its use infeasible on the edge, that's why a fork of Apache OpenWhisk has been created to make the platform more lean, the fork has been called "Lean OpenWhisk", this fork removes the need of the \textit{Kafka} server by using instead an in-memory queue \cite{lean-openwhisk}. 

Unfortunately this lighter version has been discontinued, and it only uses an old "incubator" (beta) version of OpenWhisk.


\subsection{Fission}
This active and well supported FaaS platform currently supports NodeJS, Python, Ruby, Go, PHP. But language-specific parts are isolated which make it extensible to any language.
Fission works on top of \textit{Kubernetes} (a well-known container-orchestration system) and it uses a configurable pool of containers to have a low cold-start latency ($\approx$ 100ms). Functions can auto-scale based on the CPU usage and are triggered by HTTP requests.

Internally Fission works in the following way: a stateless \textit{Router} component receives the HTTP requests (by being stateless this component can be easily scaled up or down), the \textit{Router} asks to the \textit{Executor} component the service address of the function called (this address is then stored in cache), then redirects the request to this address. The \textit{Executor} component spin up \textit{Function Pods} for running functions. A \textit{Function Pod}, when started, fetches the function information from the \textit{Kubernetes Custom Resource Definitions}, pulls the code archive and then can start serving requests that are forwarded by the \textit{Router} \cite{fission-architecture}.


\subsection{OpenFaas}
OpenFaas, differently from other projects we reported before, provides two flavours of their system. The \textit{faas} flavour allows vast scalability but comes with a bigger overhead, while the \textit{faasd} flavour cannot scale horizontally but can run on devices equipped even with small amount of memory. This allows the system to be run on hardware limited edge devices, in fact we were able to try the system even on our Raspberry Pi 3 Model B+ (a 35\$ device that has only 1 GB of RAM).

The architecture of the \textit{faas} flavour is the following: each function is built into an immutable \textit{Docker Image} and published to a \textit{Docker Registry}; when a node needs to be setup to run the function the \textit{Docker Image} is pulled from the \textit{Docker Registry} and run in a container on the \textit{Kubernetes}  container-orchestration system. In fact all the internal components are also run as containers in \textit{Kubernetes}. To allow auto-scaling the \textit{faas} system has a container running \textit{Prometheus} (a well-known open-source monitoring system), from which a component called \textit{AlertManager} reads the usage metrics (requests per second) in order to know when to scale up or down.

Instead \textit{faasd} is different in the following way: rather than using \textit{Kubernetes}, with \textit{faasd} containers are run on \textit{containerd} (a container runtime daemon). Also \textit{faasd} does not allow to have more than one replica of the container running the function, so it does not scale up.


\subsection{Other Platforms}
We found and list here also other platforms which we will not explain in details since all of them have been discontinued or are not very utilized:
\begin{itemize}
    \item \textbf{OpenLambda}: a research project with the goal of "enabling exploration of new approaches to serverless computing" \cite{openlambda};
    \item \textbf{Fn Project}: the platform is an evolution of the IronFunctions project developed by the company Iron; the development of Fn Project seem to have stopped in 2019.
    \item \textbf{Qinling}: Qinling has been developed by the creators of OpenStack, its goal was allowing Function-as-a-Service in OpenStack; unfortunately the project has been retired in 2020.
\end{itemize}

