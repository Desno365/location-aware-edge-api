\chapter{Existing Solutions}
\label{ch:existing-solutions}

In this chapter we present the current frameworks publicly available in the industry to perform \textbf{serverless computations on the edge} on a large scale.

We will show the discrepancy between the approaches proposed in research and in industry. Research frequently assumes the possibility of running virtual machines or long-running containers, which can be used for example to set up a stream processing architecture. However, real-world web infrastructure companies do not provide such capabilities due to the limited resources available in the edge.

Finally, in this chapter we will also show the latest frameworks that allow the setup of a \textbf{FaaS architecture}.



\section{Serverless Edge Computing}

In this section we show the serverless frameworks made publicly available by the industry to perform edge computations. For each of them we analyze the stateful support that the company offers.


\subsection{AWS Lambda@Edge}
With AWS Lambda@Edge it is possible to run code in a serverless manner on the edge network of AWS \cite{aws-lambda-at-edge}. Currently the only stateful support they offer publicly is with CloudFront, which is the Amazon Content Delivery Network. Therefore with this system, only \textbf{caching, stateless or forwarding} use cases can be fulfilled.


\subsection{Cloudflare Workers}
Cloudflare Workers allow developers to run serverless code across the globe on the edge network of Cloudflare \cite{cloudflare-workers}. Workers are severely capped in terms of CPU and memory usage (max 50ms CPU time limit and max 128MB of RAM), so they are not intended for a CPU intensive task, but they are extremely fast even during cold starts (a cold start is the phenomenon in which a function that was not used in a long time need to be re-instantiated). Workers in fact use an innovative technology to run code in \textit{isolates} instead of \textit{containers}, these \textit{isolates} work in a way similar to the sandbox of a web browser (e.g., Chrome) when opening a new tab and allows to have \textbf{cold starts} that last just a few milliseconds.

For Workers, Cloudflare offers publicly two types of stateful support: the Cloudflare cache system and the Workers KV. The first one is a \textbf{cache} support which exploits the Cloudflare Content Delivery Network, while Workers KV is a \textbf{global key-value data store}. Every write performed on Worker KV is propagated in an eventual consistent way to all the other edge locations, therefore Workers KV is intended for use cases with frequently read but \textbf{infrequently written} values.


\subsection{Akamai EdgeWorkers}
A competitor of Cloudflare is Akamai, and in fact the two companies provide a very similar service: also Akamai offers serverless computation on the edge equipped with the possibility of storing data with the Akamai EdgeKV, a \textbf{global key-value DB} with eventual consistent writes, perfect for use cases with number of reads greatly larger than the number of writes \cite{akamai-edge-workers}.


\subsection{Appfleet}
Appfleet allows developers to easily deploy \textbf{long-running containers} in multiple locations across the globe, with the goal of running the services closer to the users \cite{appfleet}. Unfortunately their network can't be defined as an edge network since it is currently composed of only 5 locations. However, in August 2021, Appfleet was bought by Cloudflare \cite{appfleet-cloudflare}, therefore we can expect a growth of the network in Appfleet or the introduction of support to long-running containers in Cloudflare.


\section{Solutions Summary}
Current frameworks do not provide stateful support for use cases with \textbf{frequent write operations}, we saw in fact support only for caching, stateless, forwarding or infrequent-write use cases.
If we apply the frameworks seen in the previous section to the use cases we collected and presented in Section \ref{sec:open_problems}, we can fulfill only a few use cases like the \textit{"Video Upload"} use case which is \textbf{stateless}, and the \textit{"Industrial IoT Data Compression"} use case which can employ a stateless compression. All the other use cases require an abundant rate of write operations, \textbf{making the available frameworks unsuitable} for the tasks.

Considering this situation in the industry and the need to be efficient we believe researchers should raise the effort in studying systems that can run on demand on the edge, without the need of using long-running containers.

In view of the fact that the available frameworks do not provide \textbf{write-frequent stateful support}, we tried to think of a new solution which can fulfill the use cases we collected.


\section{FaaS Platforms}
Before thinking about a new solution we studied the platforms that can be used to set up a FaaS architecture, with the idea in mind to build a prototype of our solution on this infrastructure.

Indeed we have seen in the previous sections that the \textbf{FaaS paradigm} is the most used paradigm to allow computations on the edge in the industry, while long-running solutions are not widespread, this is expected since the FaaS paradigm can allow to reach an \textbf{high efficiency} which in the edge is essential.

Therefore we studied the open-source FaaS platforms and in this section we present our findings.


\subsection{Apache OpenWhisk}
Apache OpenWhisk allows running functions with support to many different languages (e.g., Go, Java, NodeJS, .NET, PHP, Python, Scala, etc...). The project has a very active community and updates are provided periodically.
The architecture used by OpenWhisk is quite complex and internally uses a document-oriented database (\textit{CouchDB}) and a messaging platform (\textit{Kafka}) to process requests that then are forwarded to the \textit{Invoker} which runs the code inside a Docker container \cite{openwhisk-architecture}.

Apache OpenWhisk supports extreme levels of \textbf{scalability}, however this come with the cost of \textbf{size and burdensome}, in fact just running the core components, before running any actions, would require about 2.5 GB of RAM \cite{lean-openwhisk}. This make its use infeasible on the edge. Therefore a \textbf{fork of Apache OpenWhisk} has been created to make the platform more lean, the fork has been called "Lean OpenWhisk", this fork removes the need of the \textit{Kafka} server by using instead an in-memory queue \cite{lean-openwhisk}. 

Unfortunately this lighter version has been \textbf{discontinued}, and it only uses an old "incubator" (beta) version of OpenWhisk.


\subsection{Fission}
This active and well supported FaaS platform currently supports NodeJS, Python, Ruby, Go, PHP. However language-specific parts are isolated which make it extensible to any language.
Fission works on top of \textit{Kubernetes} (a well-known container-orchestration system) and it uses a configurable pool of containers to have a \textbf{low cold-start} latency ($\approx$ 100ms). Functions can auto-scale based on the CPU usage and are triggered by HTTP requests.

Internally Fission works in the following way: a stateless \textit{Router} component receives the HTTP requests (by being stateless this component can be easily scaled up or down), the \textit{Router} asks to the \textit{Executor} component the service address of the function requested (this address is then stored in cache), then redirects the request to this address. The \textit{Executor} component starts \textit{Function Pods} for running functions. A \textit{Function Pod}, when started, fetches the function information from the \textit{Kubernetes Custom Resource Definitions}, pulls the code archive and then can start serving requests that are forwarded by the \textit{Router} \cite{fission-architecture}.


\subsection{OpenFaas}
OpenFaas, differently from other projects we reported before, provides two flavours of their system. The \textit{faas} flavour allows vast \textbf{scalability} but comes with a \textbf{bigger overhead}, while the \textit{faasd} flavour \textbf{cannot scale} horizontally but can run on \textbf{hardware-limited} devices. This allows the system to be run even on edge devices equipped with a small amount of memory, in fact we were able to try the system also on our Raspberry Pi 3 Model B+ (a 35\$ device that has only 1 GB of RAM).

The architecture of the \textit{faas} flavour is the following: each function is built into an immutable \textit{Docker Image} and published to a \textit{Docker Registry}; when a node needs to be setup to run the function the \textit{Docker Image} is pulled from the \textit{Docker Registry} and run in a container on the \textit{Kubernetes}  container-orchestration system. In fact all the internal components are also run as containers in \textit{Kubernetes}. To allow \textbf{auto-scaling} the \textit{faas} system has a container running \textit{Prometheus} (a well-known open-source monitoring system), from which a component called \textit{AlertManager} reads the usage metrics (requests per second) in order to know when to scale up or down.

Instead \textit{faasd} is different in the following way: rather than using \textit{Kubernetes}, with \textit{faasd} containers are run on \textit{containerd} (a container runtime daemon). Moreover \textit{faasd} does not allow to have more than one replica of the container running the function, so it \textbf{does not scale up}.


\subsection{Other Platforms}
We found and list here also other platforms which we will not explain in details since all of them have been discontinued or are not very utilized:
\begin{itemize}
    \item \textbf{OpenLambda}: a research project with the goal of "enabling exploration of new approaches to serverless computing" \cite{openlambda};
    \item \textbf{Fn Project}: the platform is an evolution of the IronFunctions project developed by the company Iron; the development of Fn Project seems to have stopped in 2019.
    \item \textbf{Qinling}: Qinling has been developed by the creators of OpenStack, its goal was allowing Function-as-a-Service in OpenStack; unfortunately the project has been retired in 2020.
\end{itemize}
